# Методологические детали исследования рекомендательных систем

## 1. Техническая спецификация эксперимента

### 1.1 Аппаратное и программное обеспечение

**Аппаратное обеспечение:**
- Процессор: Apple M-серии / Intel x86_64
- Оперативная память: 8+ GB
- Операционная система: macOS 14.5.0

**Программное обеспечение:**
- Python 3.9.x
- Основные библиотеки:
  - scikit-learn 1.3.x (метрики, базовые алгоритмы)
  - pandas 2.0.x (обработка данных)
  - numpy 1.24.x (численные вычисления)
  - LightFM 1.17 (гибридные рекомендации)
  - implicit 0.7.x (матричная факторизация)
  - PyTorch 2.0.x (нейронные сети)
  - matplotlib 3.7.x, seaborn 0.12.x (визуализация)

### 1.2 Воспроизводимость эксперимента

**Фиксация случайности:**
```python
np.random.seed(42)
torch.manual_seed(42)
```

**Параметры разделения данных:**
- Обучающая выборка: 70% (7,738 записей)
- Тестовая выборка: 30% (3,317 записей)
- Метод разделения: временной (по полю start_time)

## 2. Детальное описание алгоритмов

### 2.1 Классические методы коллаборативной фильтрации

#### 2.1.1 K-Nearest Neighbors (KNN)
**Математическая формула:**
```
pred(u,i) = Σ(v∈N(u)) sim(u,v) × r(v,i) / Σ(v∈N(u)) |sim(u,v)|
```

**Параметры реализации:**
- Количество соседей: k = 3
- Метрика расстояния: Евклидова
- Алгоритм поиска: brute force

**Псевдокод:**
```
1. Для каждого пользователя u:
   2. Найти k ближайших соседей в пространстве предпочтений
   3. Вычислить взвешенное среднее их предпочтений
   4. Возвратить топ-N рекомендаций
```

#### 2.1.2 Popular Baseline
**Математическая формула:**
```
popularity(i) = count(i) / max(count(j) for all j)
rank(i) = argsort(popularity, descending=True)
```

**Логика работы:**
1. Подсчет частоты использования каждого сервиса
2. Нормализация по максимальной частоте
3. Ранжирование по популярности
4. Возврат топ-N популярных элементов для всех пользователей

### 2.2 Методы матричной факторизации

#### 2.2.1 Principal Component Analysis (PCA)
**Математическая формула:**
```
X ≈ U × Σ × V^T
где U - левые сингулярные векторы (пользователи)
    Σ - диагональная матрица сингулярных значений
    V - правые сингулярные векторы (элементы)
```

**Параметры:**
- Количество компонент: 10
- SVD solver: 'auto'

#### 2.2.2 Non-negative Matrix Factorization (NMF)
**Оптимизационная задача:**
```
min ||X - WH||²_F + α||W||₁ + α||H||₁
subject to: W ≥ 0, H ≥ 0
```

**Параметры:**
- Количество компонент: 10
- Инициализация: 'random'
- Random state: 42

#### 2.2.3 Alternating Least Squares (ALS)
**Итеративная оптимизация:**
```
W^(t+1) = argmin_W ||X - WH^(t)||²_F + λ||W||²_F
H^(t+1) = argmin_H ||X - W^(t+1)H||²_F + λ||H||²_F
```

**Параметры:**
- Факторы: 20
- Регуляризация: 0.1
- Итерации: 10
- Масштабирование входных данных: ×20

### 2.3 LightFM и гибридные подходы

#### 2.3.1 LightFM-WARP
**WARP Loss функция:**
```
L_WARP = Σ Σ rank_u^+(i) × [1 - s_ui + s_uj]₊
где rank_u^+(i) - ранг положительного элемента i для пользователя u
    s_ui - предсказанный скор для пары (u,i)
    [x]₊ = max(0, x)
```

**Параметры:**
- Loss: 'warp'
- Компоненты: 20
- Эпохи: 10

#### 2.3.2 PHCF-BPR (Pairwise Hybrid Collaborative Filtering)
**BPR Loss функция:**
```
L_BPR = - Σ ln σ(x̂_uij) + λ||Θ||²
где x̂_uij = x̂_ui - x̂_uj
    σ(x) = 1/(1 + e^(-x)) - сигмоид
    Θ - параметры модели
```

**Параметры:**
- Loss: 'bpr' (Bayesian Personalized Ranking)
- Компоненты: 20
- Эпохи: 5

#### 2.3.3 Гибридные подходы (KNN+LightFM)
**Комбинирование предсказаний:**
```
pred_hybrid = α × pred_lightfm + (1-α) × pred_knn
где α = 1.0 (полный вес LightFM)
```

**Алгоритм:**
1. Обучение LightFM модели на взаимодействиях
2. Обучение KNN модели на user-item матрице
3. Получение предсказаний от каждой модели
4. Взвешенное комбинирование результатов

### 2.4 Нейронные архитектуры

#### 2.4.1 Neural Collaborative Filtering (NCF)
**Архитектура:**
```
Input: user_id, item_id
↓
Embedding Layers: user_emb(64), item_emb(64)
↓
Concatenation: [user_emb; item_emb] (128)
↓
MLP: Dense(64) → ReLU → Dense(32) → ReLU → Dense(16) → ReLU
↓
Output: Dense(1) → Sigmoid
```

**Loss функция:**
```
L_NCF = -Σ [y_ui × log(ŷ_ui) + (1-y_ui) × log(1-ŷ_ui)]
Binary Cross Entropy Loss
```

**Параметры обучения:**
- Оптимизатор: Adam (lr=0.001)
- Batch size: 64
- Эпохи: 3

#### 2.4.2 DeepFM
**Архитектура:**
```
Input: user_id, item_id
↓
Embedding: user_emb(16), item_emb(16)
↓
FM Component: linear(concat_emb) 
↓
Deep Component: MLP(concat_emb) → Dense(64) → Dense(32) → Dense(1)
↓
Combination: linear_output + deep_output
↓
Output: Sigmoid(combined)
```

**Компоненты:**
- **FM часть**: моделирует взаимодействия второго порядка
- **Deep часть**: моделирует взаимодействия высшего порядка
- **Объединение**: линейная комбинация выходов

#### 2.4.3 SASRec (Self-Attentive Sequential Recommendation)
**Transformer архитектура:**
```
Input: user_id, item_id
↓
Embeddings: user_emb(16), item_emb(16)
↓
Combined: user_emb + item_emb
↓
Transformer: MultiHeadAttention(2 heads, 2 layers)
↓
Output: Linear(16→1) → Sigmoid
```

**Self-Attention механизм:**
```
Attention(Q,K,V) = softmax(QK^T/√d_k)V
где Q, K, V - запросы, ключи, значения
    d_k - размерность ключей
```

## 3. Метрики оценки

### 3.1 Precision@k
**Определение:** Доля релевантных элементов среди топ-k рекомендаций

**Формула:**
```
Precision@k = |{рекомендованные элементы} ∩ {релевантные элементы}| / k
```

**Интерпретация:** 
- Высокое значение = модель рекомендует много релевантных элементов
- Фокус на качестве рекомендаций

### 3.2 Recall@k
**Определение:** Доля найденных релевантных элементов

**Формула:**
```
Recall@k = |{рекомендованные элементы} ∩ {релевантные элементы}| / |{релевантные элементы}|
```

**Интерпретация:**
- Высокое значение = модель находит большую часть релевантных элементов
- Фокус на полноте покрытия

### 3.3 NDCG@k (Normalized Discounted Cumulative Gain)
**Формула DCG:**
```
DCG@k = Σ(i=1 to k) (2^rel_i - 1) / log₂(i + 1)
где rel_i - релевантность элемента на позиции i
```

**Нормализация:**
```
NDCG@k = DCG@k / IDCG@k
где IDCG@k - идеальный DCG для данного пользователя
```

**Интерпретация:**
- Учитывает позицию элемента в рекомендациях
- Высокие позиции более важны
- Значения от 0 до 1

### 3.4 Accuracy
**Определение:** Точность бинарной классификации

**Формула:**
```
Accuracy = (TP + TN) / (TP + TN + FP + FN)
где TP - true positives, TN - true negatives
    FP - false positives, FN - false negatives
```

### 3.5 Общий скор (Overall Score)
**Взвешенная комбинация:**
```
Overall Score = 0.4 × Precision + 0.3 × Recall + 0.3 × NDCG
```

**Обоснование весов:**
- Precision (40%): важность качества рекомендаций
- Recall (30%): важность полноты покрытия
- NDCG (30%): важность правильного ранжирования

## 4. Статистический анализ

### 4.1 Корреляционный анализ
**Метод:** Коэффициент корреляции Пирсона
```
r(X,Y) = Cov(X,Y) / (σ_X × σ_Y)
```

**Интерпретация коэффициентов:**
- |r| < 0.3: слабая корреляция
- 0.3 ≤ |r| < 0.7: умеренная корреляция  
- |r| ≥ 0.7: сильная корреляция

### 4.2 Статистические тесты
**Парный t-тест для сравнения алгоритмов:**
```
H₀: μ₁ = μ₂ (средние равны)
H₁: μ₁ ≠ μ₂ (средние различны)
```

**Критерий значимости:** p < 0.05

### 4.3 Описательная статистика
**Показатели центральной тенденции:**
- Среднее арифметическое
- Медиана
- Мода (для категориальных данных)

**Показатели разброса:**
- Стандартное отклонение
- Межквартильный размах
- Коэффициент вариации

## 5. Валидация и тестирование

### 5.1 Методология разделения данных
**Временное разделение:**
1. Сортировка записей по времени (start_time)
2. Первые 70% - обучающая выборка
3. Последние 30% - тестовая выборка

**Преимущества:**
- Имитирует реальные условия
- Избегает data leakage
- Учитывает временную эволюцию предпочтений

### 5.2 Cross-validation
**Ограничения данного исследования:**
- Небольшой размер данных (19 пользователей)
- Временная структура данных
- Cross-validation не применялся из-за специфики данных

### 5.3 Проверка на переобучение
**Мониторинг качества:**
- Сравнение метрик на train/test
- Анализ сложности моделей
- Регуляризация в нейронных моделях

## 6. Ограничения и угрозы валидности

### 6.1 Внутренняя валидность
**Потенциальные угрозы:**
- Размер выборки (19 пользователей)
- Специфика предметной области (сервисы)
- Временной период данных

**Меры по снижению:**
- Фиксация random seed
- Строгое разделение train/test
- Документирование всех параметров

### 6.2 Внешняя валидность
**Ограничения обобщаемости:**
- Специфическая предметная область
- Размер данных
- Культурные особенности пользователей

**Рекомендации:**
- Тестирование на других датасетах
- Адаптация параметров под новые домены
- Кросс-доменная валидация

### 6.3 Конструктная валидность
**Адекватность метрик:**
- Precision/Recall адекватны для задачи ранжирования
- NDCG учитывает позиционную важность
- Accuracy может быть менее информативна

## 7. Вычислительная сложность

### 7.1 Временная сложность
**Обучение:**
- KNN: O(n²m) - квадратичная по пользователям
- ALS: O(knm×iter) - линейная по данным
- NCF: O(batch×epochs×nm) - зависит от размера batch

**Предсказание:**
- Popular: O(1) - константное время
- KNN: O(nm) - линейное по данным
- Neural: O(nm) - линейное по парам user-item

### 7.2 Пространственная сложность
**Хранение моделей:**
- KNN: O(n²) - матрица расстояний
- Matrix Factorization: O(k(n+m)) - факторы
- Neural: O(parameters) - веса сети

## 8. Рекомендации по воспроизведению

### 8.1 Настройка окружения
```bash
# Создание виртуального окружения
python -m venv rec_env
source rec_env/bin/activate  # Linux/Mac
# rec_env\Scripts\activate   # Windows

# Установка зависимостей
pip install pandas numpy scikit-learn lightfm implicit torch matplotlib seaborn openpyxl
```

### 8.2 Структура проекта
```
rec-system-cf/
├── data/
│   └── calls.csv
├── src/
│   ├── refactored_complete_comparison.py
│   ├── advanced_analysis_visualization.py
│   └── precision_analysis.py
├── results/
│   ├── *.png (графики)
│   ├── *.xlsx (таблицы)
│   └── *.csv (данные)
└── docs/
    ├── scientific_analysis_report.md
    └── methodology_details.md
```

### 8.3 Порядок выполнения
1. Подготовка данных и окружения
2. Запуск основного сравнения: `python refactored_complete_comparison.py`
3. Создание расширенной визуализации: `python advanced_analysis_visualization.py`
4. Анализ precision: `python precision_analysis.py`
5. Изучение результатов в созданных файлах

---

*Документ подготовлен для обеспечения полной воспроизводимости исследования и понимания методологических решений.*